{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffeba47",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Long Short-Term Memory (LSTM)\n",
    ":label:`sec_lstm`\n",
    "\n",
    "\n",
    "Shortly after the first Elman-style RNNs were trained using backpropagation \n",
    ":cite:`elman1990finding`, the problems of learning long-term dependencies\n",
    "(owing to vanishing and exploding gradients)\n",
    "became salient, with Bengio and Hochreiter \n",
    "discussing the problem\n",
    ":cite:`bengio1994learning,Hochreiter.Bengio.Frasconi.ea.2001`.\n",
    "Hochreiter had articulated this problem as early \n",
    "as 1991 in his Master's thesis, although the results \n",
    "were not widely known because the thesis was written in German.\n",
    "While gradient clipping helps with exploding gradients, \n",
    "handling vanishing gradients appears \n",
    "to require a more elaborate solution. \n",
    "One of the first and most successful techniques \n",
    "for addressing vanishing gradients \n",
    "came in the form of the long short-term memory (LSTM) model \n",
    "due to :citet:`Hochreiter.Schmidhuber.1997`. \n",
    "LSTMs resemble standard recurrent neural networks \n",
    "but here each ordinary recurrent node\n",
    "is replaced by a *memory cell*.\n",
    "Each memory cell contains an *internal state*,\n",
    "i.e., a node with a self-connected recurrent edge of fixed weight 1,\n",
    "ensuring that the gradient can pass across many time steps \n",
    "without vanishing or exploding.\n",
    "\n",
    "The term \"long short-term memory\" comes from the following intuition.\n",
    "Simple recurrent neural networks \n",
    "have *long-term memory* in the form of weights.\n",
    "The weights change slowly during training, \n",
    "encoding general knowledge about the data.\n",
    "They also have *short-term memory*\n",
    "in the form of ephemeral activations,\n",
    "which pass from each node to successive nodes.\n",
    "The LSTM model introduces an intermediate type of storage via the memory cell.\n",
    "A memory cell is a composite unit, \n",
    "built from simpler nodes \n",
    "in a specific connectivity pattern,\n",
    "with the novel inclusion of multiplicative nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36de0ce0",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1db29",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## Gated Memory Cell\n",
    "\n",
    "Each memory cell is equipped with an *internal state*\n",
    "and a number of multiplicative gates that determine whether\n",
    "(i) a given input should impact the internal state (the *input gate*),\n",
    "(ii) the internal state should be flushed to $0$ (the *forget gate*),\n",
    "and (iii) the internal state of a given neuron \n",
    "should be allowed to impact the cell's output (the *output* gate). \n",
    "\n",
    "\n",
    "### Gated Hidden State\n",
    "\n",
    "The key distinction between vanilla RNNs and LSTMs\n",
    "is that the latter support gating of the hidden state.\n",
    "This means that we have dedicated mechanisms for\n",
    "when a hidden state should be *updated* and\n",
    "also for when it should be *reset*.\n",
    "These mechanisms are learned and they address the concerns listed above.\n",
    "For instance, if the first token is of great importance\n",
    "we will learn not to update the hidden state after the first observation.\n",
    "Likewise, we will learn to skip irrelevant temporary observations.\n",
    "Last, we will learn to reset the latent state whenever needed.\n",
    "We discuss this in detail below.\n",
    "\n",
    "### Input Gate, Forget Gate, and Output Gate\n",
    "\n",
    "The data feeding into the LSTM gates are\n",
    "the input at the current time step and\n",
    "the hidden state of the previous time step,\n",
    "as illustrated in :numref:`fig_lstm_0`.\n",
    "Three fully connected layers with sigmoid activation functions\n",
    "compute the values of the input, forget, and output gates.\n",
    "As a result of the sigmoid activation,\n",
    "all values of the three gates\n",
    "are in the range of $(0, 1)$.\n",
    "Additionally, we require an *input node*,\n",
    "typically computed with a *tanh* activation function. \n",
    "Intuitively, the *input gate* determines how much\n",
    "of the input node's value should be added \n",
    "to the current memory cell internal state.\n",
    "The *forget gate* determines whether to keep\n",
    "the current value of the memory or flush it. \n",
    "And the *output gate* determines whether \n",
    "the memory cell should influence the output\n",
    "at the current time step. \n",
    "\n",
    "\n",
    "![Computing the input gate, the forget gate, and the output gate in an LSTM model.](../img/lstm-0.svg)\n",
    ":label:`fig_lstm_0`\n",
    "\n",
    "Mathematically, suppose that there are $h$ hidden units, \n",
    "the batch size is $n$, and the number of inputs is $d$.\n",
    "Thus, the input is $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ \n",
    "and the hidden state of the previous time step \n",
    "is $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$. \n",
    "Correspondingly, the gates at time step $t$\n",
    "are defined as follows: the input gate is $\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$, \n",
    "the forget gate is $\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$, \n",
    "and the output gate is $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$. \n",
    "They are calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xi}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hi}} + \\mathbf{b}_\\textrm{i}),\\\\\n",
    "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xf}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hf}} + \\mathbf{b}_\\textrm{f}),\\\\\n",
    "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xo}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{ho}} + \\mathbf{b}_\\textrm{o}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}_{\\textrm{xi}}, \\mathbf{W}_{\\textrm{xf}}, \\mathbf{W}_{\\textrm{xo}} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{\\textrm{hi}}, \\mathbf{W}_{\\textrm{hf}}, \\mathbf{W}_{\\textrm{ho}} \\in \\mathbb{R}^{h \\times h}$ are weight parameters \n",
    "and $\\mathbf{b}_\\textrm{i}, \\mathbf{b}_\\textrm{f}, \\mathbf{b}_\\textrm{o} \\in \\mathbb{R}^{1 \\times h}$ are bias parameters.\n",
    "Note that broadcasting \n",
    "(see :numref:`subsec_broadcasting`)\n",
    "is triggered during the summation.\n",
    "We use sigmoid functions \n",
    "(as introduced in :numref:`sec_mlp`) \n",
    "to map the input values to the interval $(0, 1)$.\n",
    "\n",
    "\n",
    "### Input Node\n",
    "\n",
    "Next we design the memory cell. \n",
    "Since we have not specified the action of the various gates yet, \n",
    "we first introduce the *input node* \n",
    "$\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$.\n",
    "Its computation is similar to that of the three gates described above, \n",
    "but uses a $\\tanh$ function with a value range for $(-1, 1)$ as the activation function. \n",
    "This leads to the following equation at time step $t$:\n",
    "\n",
    "$$\\tilde{\\mathbf{C}}_t = \\textrm{tanh}(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xc}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hc}} + \\mathbf{b}_\\textrm{c}),$$\n",
    "\n",
    "where $\\mathbf{W}_{\\textrm{xc}} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{\\textrm{hc}} \\in \\mathbb{R}^{h \\times h}$ are weight parameters and $\\mathbf{b}_\\textrm{c} \\in \\mathbb{R}^{1 \\times h}$ is a bias parameter.\n",
    "\n",
    "A quick illustration of the input node is shown in :numref:`fig_lstm_1`.\n",
    "\n",
    "![Computing the input node in an LSTM model.](../img/lstm-1.svg)\n",
    ":label:`fig_lstm_1`\n",
    "\n",
    "\n",
    "### Memory Cell Internal State\n",
    "\n",
    "In LSTMs, the input gate $\\mathbf{I}_t$ governs \n",
    "how much we take new data into account via $\\tilde{\\mathbf{C}}_t$ \n",
    "and the forget gate $\\mathbf{F}_t$ addresses \n",
    "how much of the old cell internal state $\\mathbf{C}_{t-1} \\in \\mathbb{R}^{n \\times h}$ we retain. \n",
    "Using the Hadamard (elementwise) product operator $\\odot$\n",
    "we arrive at the following update equation:\n",
    "\n",
    "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.$$\n",
    "\n",
    "If the forget gate is always 1 and the input gate is always 0, \n",
    "the memory cell internal state $\\mathbf{C}_{t-1}$\n",
    "will remain constant forever, \n",
    "passing unchanged to each subsequent time step.\n",
    "However, input gates and forget gates\n",
    "give the model the flexibility of being able to learn \n",
    "when to keep this value unchanged\n",
    "and when to perturb it in response \n",
    "to subsequent inputs. \n",
    "In practice, this design alleviates the vanishing gradient problem,\n",
    "resulting in models that are much easier to train,\n",
    "especially when facing datasets with long sequence lengths. \n",
    "\n",
    "We thus arrive at the flow diagram in :numref:`fig_lstm_2`.\n",
    "\n",
    "![Computing the memory cell internal state in an LSTM model.](../img/lstm-2.svg)\n",
    "\n",
    ":label:`fig_lstm_2`\n",
    "\n",
    "\n",
    "### Hidden State\n",
    "\n",
    "Last, we need to define how to compute the output\n",
    "of the memory cell, i.e., the hidden state $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$, as seen by other layers. \n",
    "This is where the output gate comes into play.\n",
    "In LSTMs, we first apply $\\tanh$ to the memory cell internal state\n",
    "and then apply another point-wise multiplication,\n",
    "this time with the output gate.\n",
    "This ensures that the values of $\\mathbf{H}_t$ \n",
    "are always in the interval $(-1, 1)$:\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).$$\n",
    "\n",
    "\n",
    "Whenever the output gate is close to 1, \n",
    "we allow the memory cell internal state to impact the subsequent layers uninhibited,\n",
    "whereas for output gate values close to 0,\n",
    "we prevent the current memory from impacting other layers of the network\n",
    "at the current time step. \n",
    "Note that a memory cell can accrue information \n",
    "across many time steps without impacting the rest of the network\n",
    "(as long as the output gate takes values close to 0),\n",
    "and then suddenly impact the network at a subsequent time step\n",
    "as soon as the output gate flips from values close to 0\n",
    "to values close to 1. :numref:`fig_lstm_3` has a graphical illustration of the data flow.\n",
    "\n",
    "![Computing the hidden state in an LSTM model.](../img/lstm-3.svg)\n",
    ":label:`fig_lstm_3`\n",
    "\n",
    "\n",
    "\n",
    "## Implementation from Scratch\n",
    "\n",
    "Now let's implement an LSTM from scratch.\n",
    "As same as the experiments in :numref:`sec_rnn-scratch`,\n",
    "we first load *The Time Machine* dataset.\n",
    "\n",
    "### [**Initializing Model Parameters**]\n",
    "\n",
    "Next, we need to define and initialize the model parameters. \n",
    "As previously, the hyperparameter `num_hiddens` \n",
    "dictates the number of hidden units.\n",
    "We initialize weights following a Gaussian distribution\n",
    "with 0.01 standard deviation, \n",
    "and we set the biases to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fb67d9",
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class LSTMScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6e39b",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**The actual model**] is defined as described above,\n",
    "consisting of three gates and an input node. \n",
    "Note that only the hidden state is passed to the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e58c8",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LSTMScratch)\n",
    "def forward(self, inputs, H_C=None):\n",
    "    if H_C is None:\n",
    "        # Initial state with shape: (batch_size, num_hiddens)\n",
    "        H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                      device=inputs.device)\n",
    "        C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                      device=inputs.device)\n",
    "    else:\n",
    "        H, C = H_C\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
    "                        torch.matmul(H, self.W_hi) + self.b_i)\n",
    "        F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
    "                        torch.matmul(H, self.W_hf) + self.b_f)\n",
    "        O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
    "                        torch.matmul(H, self.W_ho) + self.b_o)\n",
    "        C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
    "                           torch.matmul(H, self.W_hc) + self.b_c)\n",
    "        C = F * C + I * C_tilde\n",
    "        H = O * torch.tanh(C)\n",
    "        outputs.append(H)\n",
    "    return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92381b0-2082-419d-80d7-0af93cf40fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LSTMScratch)\n",
    "def get_parameters(self):\n",
    "    return (self.W_xi.shape[0] * self.W_xi.shape[1] + \n",
    "    self.W_hi.shape[0] * self.W_hi.shape[1] +\n",
    "    self.b_i.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cea118",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "### [**Training**] and Prediction\n",
    "\n",
    "Let's train an LSTM model by instantiating the `RNNLMScratch` class from :numref:`sec_rnn-scratch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c30472",
   "metadata": {
    "origin_pos": 14,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTMScratch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mTimeMachine(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, num_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m lstm \u001b[38;5;241m=\u001b[39m \u001b[43mLSTMScratch\u001b[49m(num_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mvocab), num_hiddens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mRNNLMScratch(lstm, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mvocab), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LSTMScratch' is not defined"
     ]
    }
   ],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=128)\n",
    "lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=1024)\n",
    "model = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)\n",
    "trainer = d2l.Trainer(max_epochs=80, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b7670-0988-44e6-85a8-005c3f2607ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f908bd2-2e58-483b-ac21-1726c61b3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict('why', 100, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ad539a45-433a-48f6-87dc-ee8f3b8e6051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with has slipate at all have done so i ne dimension of space the thing s against reason said the fire fire that a mathematical man and the earth time dimension of space and so ind which are along the time traveller at and have to the other but some phind surface that a mathematical plane there is the furbele that is all have done so i admit we said the time traveller proceeded the time traveller but before the geometry of the three dimensions there is the fles there are always that is his gone about this that space as our metarkable that we can go up against deis that shaling to the german sciention of this that space as our mathematical man at right invest all there is al allol not that than a small clock and the bubbling the medical man a cube that travel the fire that our consciousness moves along the medical man and the whore was that is all have was along the medical man and the perspiction of space and time at all have to his laboratory the provincial mayor it is space and there was a'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(self, prefix, num_preds, vocab, device=None,temperature=1):\n",
    "    \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "    state, outputs = None, [vocab[prefix[0]]]\n",
    "    for i in range(len(prefix) + num_preds - 1):\n",
    "        X = d2l.tensor([[outputs[-1]]], device=device)\n",
    "        embs = self.one_hot(X)\n",
    "        rnn_outputs, state = self.rnn(embs, state)\n",
    "        if i < len(prefix) - 1:  # Warm-up period\n",
    "            outputs.append(vocab[prefix[i + 1]])\n",
    "        else:  # Predict num_preds steps\n",
    "            with torch.no_grad():\n",
    "                Y = self.output_layer(rnn_outputs).squeeze()\n",
    "                Y = torch.nn.functional.softmax(Y/temperature, dim=0)\n",
    "            l = Y.squeeze().tolist()\n",
    "            t = sum(l)\n",
    "            l = [i/t for i in l]\n",
    "            #print(l)\n",
    "            outputs.append(int(np.random.choice(len(data.vocab), p=l)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "predict(model, \"with \", 1000, data.vocab,device=d2l.try_gpu(),temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8379a858-13f8-4727-877b-fafc99ad3238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21,  9,  6,  0, 21, 10, 14,  6,  0, 14,  2,  4,  9, 10, 15,  6,  0,  3,\n",
       "        26,  0,  9,  0,  8,  0, 24,  6, 13, 13, 20,  0, 10,  0, 21,  9,  6,  0,\n",
       "        21, 10, 14,  6,  0, 21, 19,  2, 23,  6, 13, 13,  6, 19,  0,  7, 16, 19,\n",
       "         0, 20, 16,  0, 10, 21,  0, 24, 10, 13])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20deca0-1c18-4131-a0c9-8bb0bb35ca9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "82e0ee8c-ca3f-44a7-b67a-d6c236cec2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why three dimensions of space of the three dimensions of space dimensions of space dimensions of space ',\n",
       " 'why three dimensions of space of the three dimensions of space but you cannot move about in time that i',\n",
       " 'why three dimensions of space of the three dimensions of space dimensions of space dimension of space t',\n",
       " 'why three dimensions of space of the three dimensions of space dimensions of space dimension of space a',\n",
       " 'why three dimensions of space of the three dimensions of space but you cannot move about in this to the',\n",
       " 'why three dimensions of space of the three dimensions of space but you cannot move about in time there ',\n",
       " 'why three dimensions of space of the three dimensions of space but you cannot move about in time that s',\n",
       " 'why three dimensions of space of the three dimensions of space but you cannot move about in time that t',\n",
       " 'why three dimensions of space of the three dimensions of space dimensions of space dimension of space i',\n",
       " 'why three dimensions of space of the three dimensions of space dimensions of space dimension of space b']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_one_word(self, state, word, vocab, device=None, k = 5,temperature=1):\n",
    "    \"\"\"Given the prefix encoded in state tensor and current word, predict next word recursively by beam search\"\"\"\n",
    "    X = d2l.tensor([[data.vocab[word]]], device=device)\n",
    "    embs = self.one_hot(X)\n",
    "    rnn_outputs, state = self.rnn(embs, state)\n",
    "    Y = torch.nn.functional.log_softmax(self.output_layer(rnn_outputs).squeeze()/temperature,dim=0)\n",
    "    frequency_list = []\n",
    "    for i in range(len(vocab)):\n",
    "        char = data.vocab.idx_to_token[i]\n",
    "        prop = float(Y[i])\n",
    "        frequency_list.append((char, prop))\n",
    "    frequency_list.sort(key=lambda x:x[1],reverse=True)\n",
    "    return frequency_list[:k], state\n",
    "def beam_search(self, prefix, num_preds, vocab, device=None, k = 5, temperature=1):\n",
    "    # Warm up and feed all characters into nn\n",
    "    state = None\n",
    "    for i in range(len(prefix) - 1):\n",
    "        X = d2l.tensor([[vocab[prefix[i]]]], device=device)\n",
    "        embs = self.one_hot(X)\n",
    "        _, state = self.rnn(embs, state)\n",
    "    curList = [( prefix, state, 1)]\n",
    "    \n",
    "    for i in range(num_preds):\n",
    "        newlist = []\n",
    "        for node in curList:\n",
    "            prefix, state, prob = node\n",
    "            nextChar = prefix[-1]\n",
    "            frequency_list, state = predict_one_word(self, state, nextChar, vocab, device, k,temperature)\n",
    "            for (nextChar, nextProp) in frequency_list:\n",
    "                newlist.append((prefix + nextChar, state, prob + nextProp))\n",
    "        newlist.sort(key=lambda x:x[2], reverse=True)\n",
    "        curList = newlist[:k]\n",
    "    return [prefix for (prefix, _, _) in curList]\n",
    "beam_search(model, \"why\",  100, data.vocab, d2l.try_gpu(),k=10,temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f312931-66bc-4bcc-bd13-0b59dcb2eb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8fbbc-41c0-4321-9d09-de38d2134d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model,'i', 1000, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab578363",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "## [**Concise Implementation**]\n",
    "\n",
    "Using high-level APIs,\n",
    "we can directly instantiate an LSTM model.\n",
    "This encapsulates all the configuration details \n",
    "that we made explicit above. \n",
    "The code is significantly faster as it uses \n",
    "compiled operators rather than Python\n",
    "for many details that we spelled out before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3a64895-b401-4be5-aa16-01045b2eb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(self):\n",
    "    # Adam optimizer is used here\n",
    "    return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "d2l.Module.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77c6e0d7",
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class LSTM(d2l.RNN):\n",
    "    def __init__(self, num_inputs, embedding_size, num_hiddens,num_layers, dropout=0, lr=0.01):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_inputs, embedding_dim=embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, num_hiddens,num_layers, dropout=0,batch_first=True)\n",
    "        self.lr=lr\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        return self.rnn(inputs, H_C)\n",
    "    def configure_optimizers(self):\n",
    "        # Adam optimizer is used here\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1be06e93",
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "data = d2l.TimeMachine(batch_size=1024, num_steps=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ea74875-a1e5-48a3-8568-ecabd1e2d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfigure_optimizers())\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Code/transformer-test/.env/lib/python3.11/site-packages/d2l/torch.py:285\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Code/transformer-test/.env/lib/python3.11/site-packages/d2l/torch.py:298\u001b[0m, in \u001b[0;36mTrainer.fit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader:\n\u001b[0;32m--> 298\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Documents/Code/transformer-test/.env/lib/python3.11/site-packages/d2l/torch.py:213\u001b[0m, in \u001b[0;36mModule.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m--> 213\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, l, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m l\n",
      "File \u001b[0;32m~/Documents/Code/transformer-test/.env/lib/python3.11/site-packages/d2l/torch.py:189\u001b[0m, in \u001b[0;36mModule.loss\u001b[0;34m(self, y_hat, y)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_hat, y):\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm = LSTM(num_inputs=len(data.vocab), embedding_size = 128, num_hiddens=256, num_layers=2, dropout=0.15)\n",
    "model = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=0.01)\n",
    "print(model.configure_optimizers())\n",
    "trainer = d2l.Trainer(max_epochs=1, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(lstm, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75abaa22-e295-478b-a67e-60e32f8000bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.configure_optimizers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d4563d8-7fea-4fc3-bd9c-1e83efa7103f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 4\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e2f926f",
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the model there is the furburent moment one might and his slippers show that is all lave to the other than a small clock and the fire that is all have all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all have at all lave all hav'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('i', 1000, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5fa318",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "LSTMs are the prototypical latent variable autoregressive model with nontrivial state control.\n",
    "Many variants thereof have been proposed over the years, e.g., multiple layers, residual connections, different types of regularization. However, training LSTMs and other sequence models (such as GRUs) is quite costly because of the long range dependency of the sequence.\n",
    "Later we will encounter alternative models such as Transformers that can be used in some cases.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "While LSTMs were published in 1997, \n",
    "they rose to great prominence \n",
    "with some victories in prediction competitions in the mid-2000s,\n",
    "and became the dominant models for sequence learning from 2011 \n",
    "until the rise of Transformer models, starting in 2017.\n",
    "Even Tranformers owe some of their key ideas \n",
    "to architecture design innovations introduced by the LSTM.\n",
    "\n",
    "\n",
    "LSTMs have three types of gates: \n",
    "input gates, forget gates, and output gates \n",
    "that control the flow of information.\n",
    "The hidden layer output of LSTM includes the hidden state and the memory cell internal state. \n",
    "Only the hidden state is passed into the output layer while \n",
    "the memory cell internal state remains entirely internal.\n",
    "LSTMs can alleviate vanishing and exploding gradients.\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Adjust the hyperparameters and analyze their influence on running time, perplexity, and the output sequence.\n",
    "1. How would you need to change the model to generate proper words rather than just sequences of characters?\n",
    "1. Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hidden dimension. Pay special attention to the training and inference cost.\n",
    "1. Since the candidate memory cell ensures that the value range is between $-1$ and $1$ by  using the $\\tanh$ function, why does the hidden state need to use the $\\tanh$ function again to ensure that the output value range is between $-1$ and $1$?\n",
    "1. Implement an LSTM model for time series prediction rather than character sequence prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c65b71",
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1057)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
